% !TeX program = xelatex
% !BIB program = biber
\documentclass[aspectratio=169, 12pt]{beamer}
\usetheme{usyd}

% --- Title Page Meta Data ---
\title{\large Optimal Look-back Horizon for Time Series Forecasting in Federated Learning}
\subtitle{}\vspace*{-1.5em}
\author{\noindent
  Dahao Tang,
  Nan Yang\textsuperscript{1},
  Yanli Li\textsuperscript{1},
  Zhiyu Zhu\textsuperscript{2},
  Zhibo Jin\textsuperscript{2},
  Dong Yuan\textsuperscript{1}
}\vspace*{0.5em}
\institute{\noindent
  \textsuperscript{1}University of Sydney \\
  \textsuperscript{2}University of Technology Sydney \\
}
\date{AAAI 2026}

% --- Bib File ---
\addbibresource{refs.bib}

\begin{document}

% --- Title Slide ---
\begin{frame}
  \titlepage
\end{frame}

% --- Table of Contents ---
\begin{frame}
  \tableofcontents
\end{frame}

% --- Main ---
\section{Introduction}
\begin{frame}{The Gap in Time Series Forecasting (TSF)}
  \begin{itemize}
    \item \textbf{Look-back Horizon ($H$):} The number of past time steps used as 
          input to predict future values.
    \vfill
    \item \textbf{Current Practice:} Treated as a hyperparameter selected via 
          heuristics or cross-validation.
    \vfill
    \item \textbf{Trade-off:} 
    \begin{itemize}
      \item Too short $\to$ Missed temporal dependencies.
      \item Too long $\to$ Overfitting and increased noise.
    \end{itemize}
  \end{itemize}
\end{frame}
\begin{frame}{The Gap in Federated Learning (FL)}
  \begin{itemize}
    \item Existing theory assumes \textbf{centralized, IID} data \cite{shi2024scaling}.
    \vfill
    \item \textbf{FL Reality:} Data is Non-IID, heterogeneous, and distributed.
    \vfill
    \item A single global horizon is suboptimal for diverse clients.
  \end{itemize}
\end{frame}

\begin{frame}{Contributions}
  \begin{enumerate}
    \item \textbf{Synthetic Data Generator (SDG):} A structured generative model 
          capturing core temporal patterns and client heterogeneity.
    \item \textbf{Intrinsic Space Formulation:} A geometry-preserving transformation 
          enabling rigorous comparisons across clients.
    \item \textbf{Loss Decomposition:} Analytical breakdown of error into \textit{Bayesian}
            (irreducible) and \textit{Approximation} (model) components.
    \item \textbf{Optimal Horizon ($H^*$):} Proof that total loss is unimodal, 
          allowing identification of the smallest sufficient horizon.
  \end{enumerate}
\end{frame}

\section{Preliminary}
\begin{frame}{SDG: Modeling Temporal Structures}
  We model the observation $\hat{x}_{f,t,k}$ for feature $f$, time $t$, and client $k$ as:
  \begin{equation}
    \label{eq:sdg}
    \hat{x}_{f,t,k} = \underbrace{\sum_{j=1}^{J} A_{f,j,k} \sin \left( \frac{2 \pi t}{T_{f,j,k}} + \theta_{f,j,k} \right)}_{\text{Seasonality}} 
    + \underbrace{\sum_{i=1}^{p} \phi_{k,i} x_{f,t-i,k}}_{\text{AR Memory}} 
    + \underbrace{\beta_{f,k} t}_{\text{Trend}} 
    + \underbrace{\epsilon_{f,t,k}}_{\text{Noise}}
  \end{equation}

  \begin{itemize}
    \item \textbf{Seasonality:} Defined by amplitude $A_{f,j,k}$, period $T_{f,j,k}$, 
          and phase $\theta$.
    \item \textbf{AR Memory:} Autoregressive process with client-specific 
          lag coefficients $\phi_{k,i}$.
    \item \textbf{Trend:} Linear component slope $\beta_{f,k}$.
    \item \textbf{Noise:} Gaussian innovation $\epsilon \sim \mathcal{N}(\mu_{f,k}, \sigma_{f,k}^2)$.
  \end{itemize}
\end{frame}

\begin{frame}{Capturing Feature Heterogeneity}
  Real-world FL data exhibits feature skew. We model this via affine transformations:
  \begin{equation}
    x_{f,t,k} = \Lambda_{f,k} \tilde{x}_{f,t,k} + \delta_{f,k}
  \end{equation}
  \textbf{Explanation of Terms:}
  \begin{itemize}
    \item $\Lambda_{f,k}$ (Linear Scale): Controls the variance $\sigma_f^2$ for feature $f$ on client $k$.
    \item $\delta_{f,k}$ (Mean Shift): Adjusts the mean $\mu_f$ for feature $f$ on client $k$.
  \end{itemize}
  \vspace{0.5em}
  \textit{Significance:} This allows the framework to explicitly account for non-IID distributions where clients observe different scales of the same underlying features.
\end{frame}

\begin{frame}{Intrinsic Dimension Estimation}
  We map windows to an intrinsic space. The dimension $d_{I,k}(H)$ approximates the effective degrees of freedom:
  \begin{equation}
    d_{I,k}(H) \approx F \cdot \left( \min\{H, \ell_{\mathrm{AR},k}\} + g_k(H) + 1 \right)
  \end{equation}
  \textbf{Components:}
  \begin{itemize}
    \item $F$: Number of features.
    \item $\ell_{\mathrm{AR},k}$: Effective AR memory length (how far back history matters).
    \item $g_k(H)$: Resolved seasonal complexity.
    \item $+1$: Accounts for the linear trend component.
  \end{itemize}
\end{frame}

\begin{frame}{Detailed Intrinsic Components}
  \textbf{1. Effective AR Memory ($\ell_{\mathrm{AR},k}$):}
  \begin{equation}
      \ell_{\mathrm{AR}, k} = \left\lceil \frac{\ln(1 / (1 - \epsilon))}{- \ln \rho_k} \right\rceil
  \end{equation}
  \begin{itemize}
      \item $\rho_k$: Spectral radius of the AR companion matrix (stability metric).
      \item Represents the time steps needed for impulse response to decay.
  \end{itemize}

  \vspace{0.5em}
  \textbf{2. Seasonal Complexity ($g_k(H)$):}
  \begin{equation}
      g_k(H) = 2 \sum_{j=1}^{J} w_{j,k} \cdot \min \left(1, \frac{H}{T_{j,k}^*} \right)
  \end{equation}
  \begin{itemize}
      \item Measures how many full seasonal cycles fit within horizon $H$.
      \item Saturates when $H$ exceeds the period $T_{j,k}^*$.
  \end{itemize}
\end{frame}

% =============================================================================
% SECTION 4: LOSS DECOMPOSITION
% =============================================================================
\section{Loss Analysis}

\begin{frame}{Theorem 1: Federated Loss Decomposition}
  The total server-side loss for a predictor $m$ decomposes into two distinct sources of error

  \begin{equation}
      L(H,S;m) = \underbrace{L_{\mathrm{Bayes}}(H,S)}_{\text{Irreducible}} + \underbrace{L_{\mathrm{approx}}(H,S;m)}_{\text{Reducible}}
  \end{equation}

  \begin{itemize}
      \item \textbf{Bayesian Loss ($L_{\mathrm{Bayes}}$):} The error of an ideal predictor with full knowledge of the data distribution. It reflects inherent uncertainty (noise).
      \item \textbf{Approximation Loss ($L_{\mathrm{approx}}$):} The excess error due to using a finite-capacity model trained on finite samples, relative to the Bayes-optimal predictor.
  \end{itemize}
\end{frame}

\begin{frame}{Theorem 2: Client-wise Bayesian Loss}
  For client $k$, the irreducible loss is the sum of component-wise errors:

  \begin{equation}
      L_{\mathrm{Bayes}}^{(k)}(H,S) = L_{\mathrm{AR}}^{(k)}(S) + L_{\mathrm{seas}}^{(k)}(H) + L_{\mathrm{trend}}^{(k)}(H)
  \end{equation}

  \textbf{Behavior with respect to Horizon ($H$):}
  \begin{itemize}
      \item \textbf{Decreases:} As $H$ increases, we resolve more seasonal phases and trend direction.
      \item \textbf{Saturates:} Once $H$ covers the AR memory and seasonal periods, adding more history provides \textbf{zero} additional information gain.
      \item \textbf{Limit:} $\Delta L_{\mathrm{Bayes}}^{(k)}(H) \to 0$ for large $H$.
  \end{itemize}
\end{frame}

\begin{frame}{Theorem 3: Approximation Loss Bound}
  The approximation error is bounded by curvature (bias) and variance terms:

  \begin{equation}
      L_{\mathrm{approx}}^{(k)}(H,S; m) \le \underbrace{\Big( K_2^2\, d_{I,k}(H)^2 \Big)^{\frac{d_{I,k}(H)}{4 + d_{I,k}(H)}}}_{\text{Curvature / Bias Term}} 
      + \underbrace{\Big( \frac{d_{I,k}(H)\, H}{D_k} \Big)^{\frac{4}{4 + d_{I,k}(H)}}}_{\text{Variance / Finite Sample Term}}
  \end{equation}

  \textbf{Why does this increase with $H$?}
  \begin{enumerate}
      \item \textbf{Intrinsic Dimension ($d_{I,k}$):} Grows with $H$, making the function harder to learn (Curse of Dimensionality).
      \item \textbf{Effective Samples ($D_k/H$):} As window length $H$ grows, the number of independent samples in a fixed dataset decreases, increasing variance.
  \end{enumerate}
\end{frame}

\section{Optimal Horizon Selection}
\begin{frame}{The Fundamental Trade-off}
  The total loss $L^{(k)}(H)$ is \textbf{unimodal} (U-shaped).

  \begin{itemize}
      \item \textbf{Small $H$:} High Bayesian Loss (Underfitting the dynamics).
      \item \textbf{Large $H$:} High Approximation Loss (Overfitting / Insufficient samples).
  \end{itemize}

  \begin{block}{Smallest Sufficient Horizon ($H_k^*$)}
    Defined as the smallest $H$ where Bayesian loss improvements saturate within a tolerance $\delta$:
    \begin{equation}
        H_k^*(\delta) := \min\{H : |\Delta L_{\mathrm{Bayes}}^{(k)}(H)| \le \delta\}
    \end{equation}
  \end{block}
\end{frame}

\begin{frame}{Practical Selection: Seasonal Coverage}
  We can link the tolerance $\delta$ to interpretable seasonal coverage:

  \begin{equation}
      H_k^*(\delta) = \max\{\ell_{\mathrm{AR},k},\ T_k^{(\tau)}\}
  \end{equation}

  \textbf{Interpretation:}
  \begin{itemize}
      \item The optimal horizon is simply the maximum of the \textbf{effective AR memory} and the \textbf{seasonal period} required to capture $(1-\tau)$ of the signal energy.
      \item This provides a theoretically grounded, calculable target for each client.
  \end{itemize}
\end{frame}

\begin{frame}{Robust Global Horizon}
  In Federated Learning, we need a single global horizon $H_{\text{server}}$ despite heterogeneous local optima. We use a weighted trimmed mean:

  \begin{equation}
      H_{\mathrm{server}} = \operatorname{TrimMean}_{\alpha}\left(\{H_k^*(\delta)\}_{k=1}^K ; \{w_k\}_{k=1}^K\right)
  \end{equation}

  \begin{itemize}
      \item \textbf{Weights $w_k$:} Proportional to client data size ($n_k$).
      \item \textbf{Trimmed Mean:} Discards the top/bottom $\alpha$-fraction of extreme horizons.
      \item \textbf{Benefit:} Robustness. Prevents a single client with an extremely long seasonality (requiring huge $H$) from degrading the sample efficiency for all other clients.
  \end{itemize}
\end{frame}

% =============================================================================
% SECTION 9: CONCLUSION
% =============================================================================
\section{Conclusion}

\begin{frame}{Summary \& Impact}
  \begin{itemize}
      \item \textbf{Theoretical Foundation:} We provided the first rigorous derivation of optimal horizons for non-IID Federated Learning.
      \item \textbf{Key Insight:} The optimal horizon is the \textbf{smallest sufficient horizon}.
      \begin{itemize}
          \item It is \textit{not} "the longer the better."
          \item It occurs when the marginal gain in information (Bayesian) is overtaken by the marginal cost of complexity (Approximation).
      \end{itemize}
      \item \textbf{Methodology:} 
      \begin{itemize}
          \item Synthetic Data Generator (SDG) for theory.
          \item Intrinsic Space for geometry.
          \item Loss Decomposition for analysis.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \centering
  \Huge \textbf{Thank You} \\
  \vspace{1cm}
  \large \textbf{Questions?}
\end{frame}

% --- References ---
\section{Reference}
\begin{frame}[allowframebreaks]
  \frametitle{References}
  \printbibliography
\end{frame}

\end{document}