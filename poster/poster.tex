\documentclass{beamer}

\usepackage[orientation=portrait,size=a0,scale=1.0]{beamerposter}

\mode<presentation>{\usetheme{usydposter}}

%========================================================
% Core packages
%========================================================
\usepackage[australian]{babel}
\usepackage{amsmath}
\usepackage{graphicx}

\graphicspath{{assets/}{./}}

%========================================================
% Headbar logos
%========================================================
\logoleft{%
  \hspace{1.5em}
  \includegraphics[height=3.6cm]{AAAILogo}%
}

\logoright{%
  \includegraphics[height=5cm]{USYDLogo}\hspace{1.5em}%
  \includegraphics[height=5cm]{UTSLogo}%
}

%========================================================
% Footer (Contact strip)
%========================================================
\setbeamertemplate{footline}{
  \begin{beamercolorbox}[wd=\paperwidth]{upper separation line foot}
    \rule{0pt}{3pt}
  \end{beamercolorbox}
  \leavevmode%
  \begin{beamercolorbox}[ht=7ex,leftskip=1em,rightskip=1em]{author in head/foot}%
    \hfill
    \small\texttt{dahao.tang@sydney.edu.au}\hspace{2em}
    \vskip1ex
  \end{beamercolorbox}
}

%========================================================
% Title Metadata
%========================================================
\title{Optimal Look-back Horizon for Time Series Forecasting \\
       in Federated Learning}

\author{
  Dahao Tang\textsuperscript{1},
  Nan Yang\textsuperscript{1},
  Yanli Li\textsuperscript{1},
  Zhiyu Zhu\textsuperscript{2},
  Zhibo Jin\textsuperscript{2},
  Dong Yuan\textsuperscript{1}
}

\institute{
  \textsuperscript{1}The University of Sydney \qquad
  \textsuperscript{2}University of Technology Sydney

  dahao.tang@sydney.edu.au,
  n.yang@sydney.edu.au,
  yanli.li@sydney.edu.au,
  zhiyu.zhu@student.uts.edu.au,
  zhibo.jin@student.uts.edu.au,
  dong.yang@sydney.edu.au
}

% Suppress hyperref warnings for superscripts
\pdfstringdefDisableCommands{\def\textsuperscript#1{#1}}

%========================================================
% Document Layout
%========================================================
\setlength{\columnsep}{2.2cm}

\begin{document}

\begin{frame}[t]{}
  \begin{columns}[t,totalwidth=\textwidth]

    %----------------------------------------------------
    % Left Column
    %----------------------------------------------------
    \begin{column}{0.48\textwidth}

      \begin{block}{Abstract}
        Selecting an appropriate look-back horizon remains a fundamental challenge
        in time series forecasting, particularly in federated learning where data
        is decentralized and heterogeneous. We present a principled framework for
        adaptive horizon selection via an intrinsic-space formulation grounded in a
        structured synthetic data generator.
      \end{block}

      \begin{block}{Introduction}
        Existing scaling-law analyses typically assume centralized IID data and
        fail to account for client heterogeneity. In federated environments,
        clients differ in dynamics, noise, and temporal structure, making fixed
        horizons suboptimal.
        \begin{itemize}
          \item Client-aware intrinsic representation space
          \item Explicit Bayesian + approximation error decomposition
          \item Provably optimal, adaptive horizon criterion
        \end{itemize}
      \end{block}

      \begin{block}{Synthetic Data Generator}
        For client $k$, feature $f$, and time $t$:
        \[
          \hat{x}_{f,t,k}
          = \sum_{j} A_{f,j,k}\sin\!\left(\frac{2\pi t}{T_{f,j,k}}+\theta_{f,j,k}\right)
          + \sum_{i}\phi_{k,i}x_{f,t-i,k}
          + \beta_{f,k}t + \epsilon_{f,t,k}.
        \]
      \end{block}

    \end{column}

    %----------------------------------------------------
    % Right Column
    %----------------------------------------------------
    \begin{column}{0.48\textwidth}

      \begin{block}{Loss Decomposition}
        The global prediction loss decomposes as:
        \[
          L(H) = L_{\mathrm{Bayes}}(H) + L_{\mathrm{approx}}(H),
        \]
        separating irreducible client uncertainty from approximation error induced
        by finite samples and model capacity.
      \end{block}

      \begin{block}{Smallest Sufficient Horizon}
        For tolerance $\delta > 0$, define:
        \[
          H_k^*(\delta) = \min\{H : |\Delta L_{\mathrm{Bayes}}^{(k)}(H)| \le \delta\}.
        \]
        This is the smallest horizon at which additional history yields negligible
        irreducible error reduction.
      \end{block}

      \begin{block}{Conclusion}
        The total forecasting loss is unimodal in horizon length. The smallest
        sufficient horizon minimizes loss by balancing information identifiability
        against approximation error growth, yielding a principled, client-adaptive
        horizon selection rule.
      \end{block}

    \end{column}

  \end{columns}
\end{frame}

\end{document}